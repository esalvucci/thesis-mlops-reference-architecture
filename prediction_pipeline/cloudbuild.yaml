steps:
    # Build the component of the Kubeflow Pipeline
    # The name is the URI of the corresponding cloud builder container
  - name: 'gcr.io/cloud-builders/docker'
    # Args contain the arguments to be passed to the entry points.
    args: ['build', '-t', '$_DOCKER_CONTAINER_REGISTRY_BASE_URL/$_PROJECT_NAME/$_DATA_INGESTION:$SHORT_SHA', '.']
    dir: $_COMPONENTS_FOLDER/$_DATA_INGESTION

  # Build the component of the Kubeflow Pipeline
  - name: 'gcr.io/cloud-builders/docker'
    args: ['build', '-t', '$_DOCKER_CONTAINER_REGISTRY_BASE_URL/$_PROJECT_NAME/$_MODEL_LOADER:$SHORT_SHA', '.']
    dir: $_COMPONENTS_FOLDER/$_MODEL_LOADER
    env:
      - 'MLFLOW_TRACKING_URI=$_MLFLOW_TRACKING_URI'
  # Build the component of the Kubeflow Pipeline
  - name: 'gcr.io/cloud-builders/docker'
    args: ['build', '-t', '$_DOCKER_CONTAINER_REGISTRY_BASE_URL/$_PROJECT_NAME/$_INFERENCE_SERVICE:$SHORT_SHA', '.']
    dir: $_COMPONENTS_FOLDER/$_INFERENCE_SERVICE
    env:
      - 'MLFLOW_TRACKING_URI=$_MLFLOW_TRACKING_URI'

  # Through the kfp-cli compile the pipeline
  - name: 'gcr.io/$_PROJECT_ID/kfp-cli'
    # The 'env' property allows to pass the container environment variables
    # (which values are specified in the trigger settings)
    env:
      - 'DOCKER_CONTAINER_REGISTRY_BASE_URL=$_DOCKER_CONTAINER_REGISTRY_BASE_URL'
      - 'TAG=$SHORT_SHA'
      - 'HOST=$_ENDPOINT'
      - 'PROJECT_NAME=$_PROJECT_NAME'
      - 'DATA_INGESTION=$_DATA_INGESTION'
      - 'MODEL_LOADER=$_MODEL_LOADER'
      - 'INFERENCE_SERVICE=$_INFERENCE_SERVICE'
      - 'MLFLOW_TRACKING_URI=$_MLFLOW_TRACKING_URI'
    args:
      - '-c'

      # dsl-compile --py <PYHTON_FILE_NAME> --output <COMPILED_PIPELINE_FILENAME>
      - 'dsl-compile --py main.py --output prediction_pipeline.tar.gz'
    dir: $_PIPELINE_FOLDER

  # Through the kfp-cli upload a new version of it to the Kubeflow Pipeline specified endpoint.
  - name: 'gcr.io/$_PROJECT_ID/kfp-cli'
    args:
      - '-c'

      # kfp --endpoint <ENDPOINT_URI> pipeline upload-version -p <PIPELINE_ID> -v <VERSION_ID> <COMPILED_PIPELINE_FILENAME>
      # Note that ++ the pipeline must exist here ++. If the pipeline does not exists an exception will be thrown.
      # To get the PIPELINE_ID dinamically run the command inside the $() as -p argument
      - 'kfp --endpoint $_ENDPOINT pipeline upload-version -p
           $(kfp --endpoint $_ENDPOINT pipeline list | grep -w "${_PIPELINE_NAME}"  |
             grep -E -o -e "([a-z0-9]){8}-([a-z0-9]){4}-([a-z0-9]){4}-([a-z0-9]){4}-([a-z0-9]){12}")
         -v ${SHORT_SHA} prediction_pipeline.tar.gz'
    dir: $_PIPELINE_FOLDER

  # Through the kfp-cli trigger the new pipeline verison run.
  - name: 'gcr.io/$_PROJECT_ID/kfp-cli'
    args:
      - '-c'

      # kfp --endpoint <ENDPOINT_URI> run submit -e <EXPERIMENT_NAME> -r <RUN_ID> -p <PIPELINE_ID>
      # To get the PIPELINE_ID dinamically run the command inside the $() as -p argument
      - 'kfp --endpoint $_ENDPOINT run submit
              -e "${_PIPELINE_NAME}"
              -r ${SHORT_SHA}
              -p $(kfp --endpoint $_ENDPOINT pipeline list |
                  grep -w "${_PIPELINE_NAME}" |
                  grep -E -o -e "([a-z0-9]){8}-([a-z0-9]){4}-([a-z0-9]){4}-([a-z0-9]){4}-([a-z0-9]){12}")'
    dir: $_PIPELINE_FOLDER

# In the args property we build the image locally. We also need to push the built image to a docker container registry.
# This operation is specified in the 'images' property.
# Note the lowercase name ('images')
images:
  - '$_DOCKER_CONTAINER_REGISTRY_BASE_URL/$_PROJECT_NAME/$_DATA_INGESTION:$SHORT_SHA'
  - '$_DOCKER_CONTAINER_REGISTRY_BASE_URL/$_PROJECT_NAME/$_MODEL_LOADER:$SHORT_SHA'
